# -*- coding: utf-8 -*-
"""Rsearch_paper_summarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kRPM0xtjWvfD6y3qF0WvAhDg7CqmX8uY
"""

# !pip install spacy

# !pip install PyPDF2

import PyPDF2

def extract_abstract_from_pdf(pdf_path):
    abstract = ""

    with open(r"attention is all you need - research paper.pdf", "rb") as pdf_file:
        pdf_reader = PyPDF2.PdfReader(pdf_file)

        # Iterate through each page of the PDF
        for page_num in range(len(pdf_reader.pages)):
            page = pdf_reader.pages[page_num]
            page_text = page.extract_text()

            # Check if the page contains the word "Abstract" (case insensitive)
            if "abstract" in page_text.lower():
                # Find the index of "Abstract"
                abstract_start_idx = page_text.lower().find("abstract")

                # Extract the text from the point of "Abstract" onward
                abstract = page_text[abstract_start_idx:]

                # Break the loop once the abstract is found
                break

    return abstract

# Example usage
pdf_path = "attention is all you need - research paper.html"
abstract = extract_abstract_from_pdf(pdf_path)
print(abstract)

text=abstract

import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from string import punctuation

stopwords = list(STOP_WORDS)

nlp = spacy.load('en_core_web_sm')

doc = nlp(text)

tokens = [token.text for token in doc]
print(tokens)

punctuation = punctuation + "\n"
# punctuation

word_frequencies = {}
for word in doc:
  if word.text.lower() not in stopwords:
    if word.text.lower() not in punctuation:
      if word.text not in word_frequencies.keys():
        word_frequencies[word.text] = 1
      else:
        word_frequencies[word.text] += 1

#word_frequencies

max_frequency = max(word_frequencies.values())
# max_frequency

sentence_tokens = [sent for sent in doc.sents]

sentence_scores = {}
for sent in sentence_tokens:
  for word in sent:
    if word.text.lower() in word_frequencies.keys():
      if sent not in sentence_scores.keys():
        sentence_scores[sent] = word_frequencies[word.text.lower()]
      else:
        sentence_scores[sent] = word_frequencies[word.text.lower()]

from heapq import nlargest

select_length = int(len(sentence_tokens)*0.3)
# select_length

summary = nlargest(select_length,sentence_scores,key = sentence_scores.get)

# summary

result = " ".join([token.text for token in summary])

result = result.replace("\n"," ")

print(result)